{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3227c7-19c6-4daa-87f0-fd9ead31c5a6",
   "metadata": {},
   "source": [
    "# Tutorial of DeepAdapter\n",
    "### A self-adaptive and versatile tool for eliminating multiple undesirable variations from transcriptome\n",
    "In this notebook, you will learn how to reproduce the results and re-train your DeepAdapter with your own datasets.\n",
    "## 1. Installation and requirements\n",
    "### 1.1. Installation\n",
    "To run locally, please open a terminal window and download the code with:\n",
    "```sh\n",
    "$ # Clone this repository to your local computer\n",
    "$ git clone https://github.com/mjDelta/DeepAdapter.git\n",
    "$ cd DeepAdapter\n",
    "$ # create a new conda environment\n",
    "$ conda create -n deepAdapter python=3.9\n",
    "$ # activate environment\n",
    "$ conda activate deepAdapter\n",
    "$ # Install dependencies\n",
    "$ pip install -r requirements.txt\n",
    "$ # Launch jupyter notebook\n",
    "$ jupyter notebook\n",
    "```\n",
    "To execute a \"cell\", please press Shift+Enter\n",
    "\n",
    "### 1.2. Formats of your own input files\n",
    "Your own dataset should include $2$ files (**Note: these 2 files are put in the same directory.**): \n",
    "* **gene_expression.txt** for gene expression matrix;\n",
    "* **unwantedVar_biologicalSig.txt** for annotations of unwanted variations and biological signals.\n",
    "\n",
    "The example of **gene_expression.txt** is as follows (**Note: every row should be split by commas.**):\n",
    "| SampleId | Gene_1 | Gene_2 | Gene_3 | ... | Gene_n-2 | Gene_n-1 | Gene_n |\n",
    "|  ----  | ----  | ----  | ----  |  ----  | ----  | ----  | ----  |\n",
    "| **1** | x<sub>11</sub> | x<sub>12</sub> | x<sub>13</sub> | ... | x<sub>1(n-2)</sub> | x<sub>1(n-1)</sub> | x<sub>1n</sub> |\n",
    "| **2** | x<sub>21</sub> | x<sub>22</sub> | x<sub>23</sub> | ... | x<sub>2(n-2)</sub> | x<sub>2(n-1)</sub> | x<sub>2n</sub> |\n",
    "| ... | ... | ... | ... | ... | ... | ... | ... |\n",
    "| **m** | x<sub>m1</sub> | x<sub>m2</sub> | x<sub>m3</sub> | ... | x<sub>m(n-2)</sub> | x<sub>m(n-1)</sub> | x<sub>mn</sub> |\n",
    "\n",
    "The example of **unwantedVar_biologicalSig.txt** is as follows (**Note: every row should be split by commas.**):\n",
    "| SampleId | Unwanted_var | Biological_sig |\n",
    "|  ----  | ----  | ----  |\n",
    "| **1** | unwantedVar<sub>1</sub> | biologicalSig<sub>1</sub> |\n",
    "| **2** | unwantedVar<sub>1</sub> | biologicalSig<sub>1</sub> |\n",
    "| ... | ... | ... |\n",
    "| **m** | unwantedVar<sub>p</sub> | biologicalSig<sub>q</sub> |\n",
    "\n",
    "Examples of **unwantedVar** and **biologicalSig**:\n",
    "* **unwantedVar**:\n",
    "    * **batch**: batch1, batch2, ..., batch(n);\n",
    "    * **platform**: RNA-seq, microarray;\n",
    "    * **purity**: cell lines, tissue;\n",
    "    * ...\n",
    "* **biologicalSig**:\n",
    "    * **cancer types**: lung cancer, kidney cancer, ..., bone cancer;\n",
    "    * **lineages**: Lung, kidney, ..., eye;\n",
    "    * **donor sources**: donor1, donor2, ..., donor(n);\n",
    "    * ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db523b1-6e48-4bd1-b0f9-9eb38e40c0e1",
   "metadata": {},
   "source": [
    "## 2. Load the datasets and preprocess\n",
    "### 2.1. Load the modules\n",
    "There are three modules in DeepAdapter:\n",
    "* `models`: network structure and training process are defined here.\n",
    "* `utils`: triplet, decompostion and other utils are defined here.\n",
    "* `params`: you can revise the parameter settings here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc07082-e02d-431f-9fba-b5b78c85fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import data_utils as DT\n",
    "from utils import utils as UT\n",
    "import utils.triplet as TRP\n",
    "from models.trainer import Trainer\n",
    "from models.data_loader import TransData, DataLoader\n",
    "from models.dl_utils import AE, FBatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475fb653-5db1-4f45-b29b-344413279bce",
   "metadata": {},
   "source": [
    "### 2.2. Load the your own dataset\n",
    "Replace the **yourDataDir** with the directory where your own dataset is located in.\n",
    "\n",
    "Name the columns of sample id, unwanted variation annotations, and wanted signal annotations as <u>**SampleID**</u>, <u>**Unwanted_var**</u>, and <u>**Biological_sig**</u>, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c4e81d-f681-43bb-b299-e7953ff7e2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data_path = \"yourDataDir/gene_expression.txt\" ## the path of your gene expression matrix\n",
    "ann_data_path = \"yourDataDir/unwantedVar_biologicalSig.txt\" ## the path of your annotation information\n",
    "sample_id = \"SampleId\"\n",
    "unwanted_var_col = \"Unwanted_var\"\n",
    "wanted_sig_col = \"Biological_sig\"\n",
    "\n",
    "loadTransData = DT.LoadTransData(exp_data_path, ann_data_path, sample_id, unwanted_var_col, wanted_sig_col)\n",
    "data, ids, unwanted_labels, wanted_labels = loadTransData.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7845814c-c265-4bf1-b662-a685d52933a9",
   "metadata": {},
   "source": [
    "### 2.3. Preprocess the transcriptomic data\n",
    "The gene expression profiles are preprocessed by sample normalization, gene ranking, and log normalization. Let $S_i = \\sum_l x_{i l}$ denote the sum over all genes. In sample normalization, we divide $S_i$ for every sample and multiply a constant 10000 ([Xiaokang Yu et al. Nature communications, 2023](https://www.nature.com/articles/s41467-023-36635-5)):\n",
    "$$x_{i l} = \\frac{x_{i l}}{S_i} 10^4.$$\n",
    "Then, we sort genes by their expression levels and perform the log transformation $x_{i l} = \\log {(x_{i l} + 1)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d387d965-c88f-4aed-95cd-b6dbf2d0017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepTransData = DT.PrepTransData()\n",
    "raw_df = prepTransData.sample_norm(data)\n",
    "raw_df, sorted_cols = prepTransData.sort_genes_sgl_df(raw_df)\n",
    "input_arr = prepTransData.sample_log(raw_df)\n",
    "bat2label, label2bat, unwanted_labels, unwanted_onehot = prepTransData.label2onehot(unwanted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f9d681-58fc-4e39-a8a1-3511bf5810e0",
   "metadata": {},
   "source": [
    "## 3. Train DeepAdapter\n",
    "### 3.1. Adjust DeepAdapter's parameters\n",
    "The parameters for DeepAdapter are as follows (**Note: you can open `params.dl_params.py` and revise the parameters.**):\n",
    "* **epochs**: the total training epochs of DeepAdapter, default = $150000$\n",
    "* **ae_epochs**: the warmup epochs of autoencoder in DeepAdapter, default = $400$\n",
    "* **batch_epochs**: the warmup epochs of discriminator in DeepAdapter, default = $50$\n",
    "* **batch_size**: the batch size of dataloader, default = $256$\n",
    "* **hidden_dim**: the hidden units of autoencoder in DeepAdapter, default = $256$\n",
    "* **z_dim**: the latent units of autoencoder in DeepAdapter, default = $128$\n",
    "* **drop**: the dropout rate of DeepAdapter, default = $0.3$\n",
    "* **lr_lower_ae**: the lower learning rate of autoencoder in DeepAdapter, default = $1e-5$\n",
    "* **lr_upper_ae**: the upper learning rate of autoencoder in DeepAdapter, default = $5e-4$\n",
    "* **lr_lower_batch**: the lower learning rate of discriminator in DeepAdapter, default = $1e-5$\n",
    "* **lr_upper_batch**: the upper learning rate of discriminator in DeepAdapter, default = $5e-4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa260a0-df65-4594-8227-fd095b26fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import dl_params as DLPARAM\n",
    "net_args = DLPARAM.load_dl_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83a7db1-f4eb-4ee2-b432-51c2845fc39c",
   "metadata": {},
   "source": [
    "### 3.2. Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeb151d-5a9f-414a-a25b-4dca43a7b7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, train_labels_hot, \\\n",
    "    val_data, val_labels, val_labels_hot, \\\n",
    "    test_data, test_labels, test_labels_hot, \\\n",
    "    train_ids, val_ids, test_ids, \\\n",
    "    tot_train_val_idxs, tot_train_idxs, tot_val_idxs, tot_test_idxs = DT.data_split_random(input_arr, unwanted_labels, unwanted_onehot, ids)\n",
    "\n",
    "train_bios, val_bios, test_bios = wanted_labels[tot_train_idxs], wanted_labels[tot_val_idxs], wanted_labels[tot_test_idxs]\n",
    "bio_label2bat = {t:t for t in set(train_bios)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aaab98-836f-4f74-9b4a-e2af4c65fd52",
   "metadata": {},
   "source": [
    "### 3.3. Train DeepAdapter \n",
    "Two options are provided for training DeepAdapter. If you want to learn the training process, please train it step by step. If you want to skip these initializations, please use the one-line code :)!\n",
    "* To train DeepAdapter step by step, you need to initialize models, dataloaders, trainer, and the mutual nearest neighbors.\n",
    "* To train DeepAdapter in one-line code, just utilize `deepAdapter.run.train()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce7a975-f0d9-48ed-8968-87e40d85642e",
   "metadata": {},
   "source": [
    "#### 3.3.1. Train it step by step\n",
    "Give a name to this dataset by replace the **yourDataName** for variable **db_name**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba951b1-797d-409e-ab41-e5a0e048471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"yourDataName\"\n",
    "out_dir = os.path.join(\"model/deepAligner/stepByStep_{}/\".format(db_name))\n",
    "os.makedirs(out_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d2801b-cce2-4d53-921f-4875ec4140cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize models\n",
    "in_dim = input_arr.shape[1]\n",
    "num_unw_vars = len(bat2label)\n",
    "ae = AE(in_dim, net_args.hidden_dim, num_unw_vars, net_args.z_dim, net_args.drop).cuda()\n",
    "fbatch = FBatch(net_args.hidden_dim, num_unw_vars, net_args.z_dim, net_args.drop).cuda()\n",
    "\n",
    "## initialize dataloaders\n",
    "train_trans = TransData(train_data, train_labels, train_bios, train_ids, train_labels_hot)\n",
    "train_loader = DataLoader(train_trans, batch_size = net_args.batch_size, collate_fn = train_trans.collate_fn, shuffle = True, drop_last = False)\n",
    "val_trans = TransData(val_data, val_labels, val_bios, val_ids, val_labels_hot)\n",
    "val_loader = DataLoader(val_trans, batch_size = net_args.batch_size, collate_fn = val_trans.collate_fn, shuffle = False, drop_last = False)\n",
    "test_trans = TransData(test_data, test_labels, test_bios, test_ids, test_labels_hot)\n",
    "test_loader = DataLoader(test_trans, batch_size = net_args.batch_size, collate_fn = test_trans.collate_fn, shuffle = False, drop_last = False)\n",
    "\n",
    "## initialize trainer\n",
    "trainer = Trainer(train_loader, val_loader, test_loader, ae, fbatch, bio_label2bat, label2bat, net_args, out_dir)\n",
    "\n",
    "## initialize mutual nearest neighbors\n",
    "train_mutuals = TRP.find_MNN_cosine_kSources(train_data, train_labels, train_ids)\n",
    "val_mutuals = TRP.find_MNN_cosine_kSources(val_data, val_labels, val_ids)\n",
    "\n",
    "## begin training!\n",
    "trainer.fit(train_mutuals, val_mutuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f22106c-532f-4fdd-873c-68085aea3614",
   "metadata": {},
   "source": [
    "#### 3.3.2. Train it in one-line code\n",
    "Parameters for one-line code training:\n",
    "* **train_list**: the list of training transcriptomic profiles, unwanted variations, biological signals, data ids, and onehot representations of unwanted variations.\n",
    "* **val_list**: the list of validation transcriptomic profiles, unwanted variations, biological signals, data ids, and onehot representations of unwanted variations.\n",
    "* **test_list**: the list of testing transcriptomic profiles, unwanted variations, biological signals, data ids, and onehot representations of unwanted variations.\n",
    "* **label2unw**: the dictionary which maps unwanted labels (e.g., 0, 1 ...) to unwanted variations (e.g., batch1, batch2 ...)\n",
    "* **label2wnt**: the dictionary which maps biological labels (e.g., 0, 1 ...) to biological annotations (e.g., donor1, donor2 ...)\n",
    "* **net_args**: the parameters to construct DeepAdapter\n",
    "* **out_dir**: the out directory for saved models and logged losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6f06f3-34bd-4124-8fce-4ac24f7ad1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"yourDataName\"\n",
    "out_dir = os.path.join(\"model/deepAligner/oneLineCode_{}/\".format(db_name))\n",
    "os.makedirs(out_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa8eed2-8ef2-4105-b352-ebdbfb6763d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = [train_data, train_labels, train_bios, train_ids, train_labels_hot]\n",
    "val_list = [val_data, val_labels, val_bios, val_ids, val_labels_hot]\n",
    "test_list = [test_data, test_labels, test_bios, test_ids, test_labels_hot]\n",
    "\n",
    "from deepAdapter import run as RUN\n",
    "trainer = RUN.train(\n",
    "    train_list = train_list, \n",
    "    val_list = val_list, \n",
    "    test_list = test_list, \n",
    "    label2unw = label2bat, \n",
    "    label2wnt = bio_label2bat, \n",
    "    net_args = net_args, \n",
    "    out_dir = out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1660e05c-41e7-4673-843c-08ab3c35a9c5",
   "metadata": {},
   "source": [
    "## 4. Align the data\n",
    "### 4.1. Load trained model & quantatitive evaluation\n",
    "* Step 1: load the best-trained model\n",
    "* Step 2: utilize `trainer.evaluate()`\n",
    "\n",
    "In `trainer.evaluate()`, we perform decomposition analysis of aligned data and perform the quantatitive analysis including alignment score, ASW, NMI, and ARI calcuation. The quantatitive results are recorded in `record_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4e13a5-9df6-458d-84e1-1343ed016454",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_trained_ae(os.path.join(out_dir, \"ae.tar\"))\n",
    "\n",
    "record_path = os.path.join(out_dir, \"test_res.csv\")\n",
    "test_data, test_aligned_data, test_wnt_infs, test_unw_infs = trainer.evaluate(record_path, db_name, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe16151-a72a-4b70-8285-ea408df3f3ce",
   "metadata": {},
   "source": [
    "Additionally, you can perform any other analysis you like with the aligned data `aligned_data`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be19b2f-18f6-482e-a370-3e33436d4db3",
   "metadata": {},
   "source": [
    "### 4.2. Save the aligned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bd1296-ae08-4861-9a33-a2ea379a9e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trans = TransData(np.vstack((train_data, val_data, test_data)), \n",
    "                      np.array(list(train_labels) + list(val_labels) + list(test_labels)),\n",
    "                      np.array(list(train_bios) + list(val_bios) + list(test_bios)),\n",
    "                      np.array(list(train_ids) + list(val_ids) + list(test_ids)), \n",
    "                      np.vstack((train_labels_hot, val_labels_hot, test_labels_hot)))\n",
    "all_loader = DataLoader(all_trans, batch_size = net_args.batch_size, collate_fn = all_trans.collate_fn, shuffle = False, drop_last = False)\n",
    "record_path = os.path.join(out_dir, \"res.csv\")\n",
    "data, aligned_data, wnt_infs, unw_infs = trainer.evaluate(record_path, db_name, all_loader)\n",
    "\n",
    "save_path = os.path.join(out_dir, \"DA_data.csv\")\n",
    "df = pd.DataFrame(data, columns = sorted_cols)\n",
    "df[\"ID\"] = np.array(list(train_ids) + list(val_ids) + list(test_ids))\n",
    "df[\"wantInfo\"] = wnt_infs\n",
    "df[\"unwantInfo\"] = unw_infs\n",
    "df.to_csv(save_path, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
